# Streaming de vÃ­deos - Pipeline Completo

## ğŸ“Š VisÃ£o Geral

Este projeto implementa uma soluÃ§Ã£o completa de engenharia de dados, abrangendo desde a ingestÃ£o e transformaÃ§Ã£o de dados atÃ© a criaÃ§Ã£o de dashboards interativos para anÃ¡lise e monitoramento. A arquitetura foi desenvolvida utilizando tecnologias modernas de big data, incluindo Apache Spark, Data Lakes e ferramentas de visualizaÃ§Ã£o avanÃ§adas. A temÃ¡tica escolhida â€” plataforma de streaming de vÃ­deo â€” permite trabalhar com dados realistas e ricos em volume e variedade.

## ğŸ—ï¸ Arquitetura da SoluÃ§Ã£o

O projeto segue uma arquitetura de **Data Lake** com processamento em camadas:

- **ğŸ“¥ IngestÃ£o**: Coleta automatizada de dados de mÃºltiplas fontes
- **ğŸ¥‰ Bronze Layer**: Dados brutos no formato original (Delta/Iceberg)
- **ğŸ¥ˆ Silver Layer**: Dados limpos e estruturados
- **ğŸ¥‡ Gold Layer**: Dados agregados e otimizados para consumo
- **ğŸ“ˆ Dashboards**: VisualizaÃ§Ãµes e KPIs para tomada de decisÃ£o

## ğŸ“‹ PrÃ©-requisitos

Antes de iniciar, certifique-se de ter instalado em sua mÃ¡quina:

- **Python 3.11.9**
- **Docker Desktop** (manter aberto durante o uso)
- **Git**
- **VS Code**
- **Astro CLI**

## ğŸš€ ConfiguraÃ§Ã£o do Ambiente

### 1. PreparaÃ§Ã£o do Workspace

1. Crie a pasta `ABP-eng-de-dados` dentro da pasta `Documents`
2. Abra o VS Code na pasta `C:\Users\{seu-usuario}\Documents`

### 2. Clone do RepositÃ³rio

Execute o seguinte comando no terminal do VS Code:

```bash
git clone https://github.com/brunogmadeira/ABP-eng-de-dados.git
```

### 3. InstalaÃ§Ã£o do Astro CLI

Execute o PowerShell como **administrador** e rode o comando:

```powershell
winget install -e --id Astronomer.Astro
```

## ğŸ”§ ConfiguraÃ§Ã£o dos ServiÃ§os

### 4. InicializaÃ§Ã£o do Docker

Na pasta **raiz** do projeto, execute:

```bash
docker compose up -d
```

### 5. ConfiguraÃ§Ã£o do Airflow

1. Navegue atÃ© a pasta `astro` dentro do projeto:
   ```bash
   cd astro
   ```

2. Inicie o ambiente Airflow:
   ```bash
   astro dev start
   ```

3. Acesse o Airflow atravÃ©s do navegador na porta **8080**: `http://localhost:8080`

### 6. InstalaÃ§Ã£o de DependÃªncias Python

Execute o comando para instalar o conector MySQL:

```bash
pip install mysql-connector-python
```

### 7. IngestÃ£o de Dados

Execute o script Python para realizar a ingestÃ£o dos dados CSV:

```bash
python ingestao_dados_mysql.py
```

## ğŸ“ ObservaÃ§Ãµes Importantes

- Mantenha o **Docker Desktop** aberto durante todo o processo
- Certifique-se de estar na pasta correta ao executar cada comando
- O Airflow estarÃ¡ disponÃ­vel na porta **8080** apÃ³s a inicializaÃ§Ã£o

## ğŸ› ï¸ ResoluÃ§Ã£o de Problemas

Se encontrar algum erro durante a configuraÃ§Ã£o:

1. Verifique se todos os prÃ©-requisitos estÃ£o instalados
2. Confirme se o Docker Desktop estÃ¡ executando
3. Certifique-se de estar executando os comandos nas pastas corretas
4. Verifique se as portas necessÃ¡rias nÃ£o estÃ£o sendo utilizadas por outros serviÃ§os

---

**Desenvolvido para o curso de Engenharia de Dados**

## âœï¸ Autores

* Bruno Girardi Madeira - [brunogmadeira] (https://github.com/brunogmadeira)
* JoÃ£o Gabriel Rosso - [joaodagostin] (https://github.com/joaodagostin)
* JoÃ£o Pedro Taufembach - [JoaoAcordi] (https://github.com/JoaoAcordi)
* Rafael Frassetto - [rafafrassetto] (https://github.com/rafafrassetto)
* Yasmin Elias Michels - [YasminMichels] (https://github.com/YasminMichels)


